{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/noah/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/noah/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/noah/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Using United States server backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('punkt')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import translators as ts\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = open(\"train.txt\")\n",
    "# test = open(\"test.txt\")\n",
    "\n",
    "\n",
    "def getData(file):\n",
    "    fileSrc = open(file)\n",
    "    dataset = []\n",
    "    item = {}\n",
    "    for index, i in enumerate(fileSrc):\n",
    "        if(index%6 == 0):\n",
    "            item['source'] = i.replace(\"\\n\", \"\")\n",
    "        elif(index%6==1):\n",
    "            item['ref'] = i.replace(\"\\n\", \"\")\n",
    "        elif(index%6 ==2):\n",
    "            item['cand'] = i.replace(\"\\n\", \"\")\n",
    "        elif(index%6 ==3):\n",
    "            item['qual'] = i.replace(\"\\n\", \"\")\n",
    "        elif(index%6 ==4):\n",
    "            if i.replace(\"\\n\", \"\") == 'H':\n",
    "                item['target'] = 1\n",
    "            else:\n",
    "                item['target'] = 0\n",
    "        elif(index%6 ==5):\n",
    "            dataset.append(item)            \n",
    "            item['data'] = item['source'] + ' ' + item['cand']\n",
    "            item = {}\n",
    "    return dataset\n",
    "\n",
    "train = getData(\"train.txt\")\n",
    "# train = pd.DataFrame.from_records(train)\n",
    "\n",
    "test = getData(\"test.txt\")\n",
    "# test = pd.DataFrame.from_records(test)\n",
    "\n",
    "\n",
    "# humanScores = 0\n",
    "# humanCount = 0\n",
    "# mScores =0\n",
    "# mCount= 0\n",
    "# for i in train:\n",
    "#     if i['target'] == 1:\n",
    "#         humanScores += float(i['nist'])\n",
    "#         humanCount+=1\n",
    "#     else:\n",
    "#         mScores += float(i['nist'])\n",
    "#         mCount+=1\n",
    "        \n",
    "# print(humanCount)\n",
    "# print(mCount)\n",
    "# print(humanScores/humanCount)\n",
    "# print(mScores/mCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'backTrans': 'Domain names are the basis of communication for all Internet applications, including e-commerce and e-government.Currently, it has been widely used for Internet addresses.', 'cand': 'a domain name is the communication basis for all internet applications including the conducting of electronic business and electronic government affairs . at present , it is widely used for internet addresses .', 'ref': 'domain names are the basis of communications on which all internet sites use for e-business , e-government , and other applications . at present , they are widely used as internet addresses .', 'source': '域 名 是 开展 电子 商务 , 电子 政务 等 一切 互 联 网 应用 的 通信 基础 , 目前 被 广泛 使用 作为 互 联 网 地址 .', 'qual': 0.5455, 'target': 1, 'data': '域 名 是 开展 电子 商务 , 电子 政务 等 一切 互 联 网 应用 的 通信 基础 , 目前 被 广泛 使用 作为 互 联 网 地址 . a domain name is the communication basis for all internet applications including the conducting of electronic business and electronic government affairs . at present , it is widely used for internet addresses .'}\n"
     ]
    }
   ],
   "source": [
    "# trainPD = pd.read_csv(\"out.csv\")\n",
    "# # trainPD.drop(trainPD.index[[585,6996]])\n",
    "# # for i in range(585, 6995):\n",
    "# print(trainPD.shape)\n",
    "# for i in range(0,trainPD.shape[0]):\n",
    "#     if i >469:\n",
    "# chineseTrans = ts.google(\"a domain name is the communication basis for all internet applications including the conducting of electronic business and electronic government affairs . at present , it is widely used for internet addresses .\",to_language ='zh')\n",
    "# engTrans = ts.google(chineseTrans, to_language ='en')\n",
    "# print(engTrans)\n",
    "# # print(engTrans)\n",
    "# trainPD.at[583, 'backTrans'] = engTrans\n",
    "# trainPD.at[583, 'cand'] = \"the spokesman claimed that china had expressed its strong displeasure with the speech delivered by mr. hyde , chairman of the u.s. house international relations committee , in hong kong on december 2 , which contained malicious attacks on china's development and progress , and tarnished china's foreign policy with his cold war mentality .\"\n",
    "# trainPD.at[583, 'ref'] = \"the spokesman said the speech in hong kong on the 2 nd by mr. hyde , chairman of the us house of representatives ' international relations committee , was a malicious attack on china's development and progress as well as a slander on china's foreign policy evidencing a cold war mentality , with which we are strongly unsatisfied .\"\n",
    "# trainPD.at[583, 'source'] = \"发言人 表示 , 美国国会 众议院 国际 关系 委员会 主席 海德 先生 2 日 在 港 发表 的 演讲 , 对 中国 的 发展 与 进步 进行 了 恶意 攻击 , 并 以 冷战 思维 污蔑 中国 外交 政策 , 我们 表示 强烈 不满 .\"\n",
    "# trainPD.at[583, 'qual'] = 0.6332\n",
    "# trainPD.at[583, 'target'] = 1\n",
    "# trainPD.at[583, 'data'] = trainPD.iloc[583]['source'] + ' ' + trainPD.iloc[583]['cand']\n",
    "\n",
    "\n",
    "# # print(trainPD.iloc[i]['backTrans'])\n",
    "# trainPD.iloc[583]\n",
    "# lastItem = \n",
    "\n",
    "\n",
    "# print(engTrans)\n",
    "testObj = {}\n",
    "testObj['backTrans'] = engTrans\n",
    "testObj['cand'] =\"a domain name is the communication basis for all internet applications including the conducting of electronic business and electronic government affairs . at present , it is widely used for internet addresses .\"\n",
    "testObj['ref'] = \"domain names are the basis of communications on which all internet sites use for e-business , e-government , and other applications . at present , they are widely used as internet addresses .\"\n",
    "testObj['source'] = \"域 名 是 开展 电子 商务 , 电子 政务 等 一切 互 联 网 应用 的 通信 基础 , 目前 被 广泛 使用 作为 互 联 网 地址 .\"\n",
    "testObj['qual'] = 0.5455\n",
    "testObj['target'] = 1\n",
    "testObj['data'] = testObj['source'] + ' ' + testObj['cand']\n",
    "\n",
    "print(testObj)\n",
    "test.append(testObj)\n",
    "# print(trainPD.iloc[i]['backTrans'])\n",
    "# trainPD.iloc[583]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'backTrans': 'Domain names are the basis of communication for all Internet applications, including e-commerce and e-government.Currently, it has been widely used for Internet addresses.', 'cand': 'a domain name is the communication basis for all internet applications including the conducting of electronic business and electronic government affairs . at present , it is widely used for internet addresses .', 'ref': 'domain names are the basis of communications on which all internet sites use for e-business , e-government , and other applications . at present , they are widely used as internet addresses .', 'source': '域 名 是 开展 电子 商务 , 电子 政务 等 一切 互 联 网 应用 的 通信 基础 , 目前 被 广泛 使用 作为 互 联 网 地址 .', 'qual': 0.5455, 'target': 1, 'data': '域 名 是 开展 电子 商务 , 电子 政务 等 一切 互 联 网 应用 的 通信 基础 , 目前 被 广泛 使用 作为 互 联 网 地址 . a domain name is the communication basis for all internet applications including the conducting of electronic business and electronic government affairs . at present , it is widely used for internet addresses .'}\n"
     ]
    }
   ],
   "source": [
    "# for index, ex in enumerate(test):\n",
    "#     if index >169:\n",
    "#         print(index)\n",
    "#         chineseTrans = ts.google(ex['cand'],to_language ='zh')\n",
    "#         #     print(chineseTrans)\n",
    "#         ex['backTrans'] = ts.google(chineseTrans, to_language ='en')\n",
    "#         print(ex['backTrans'])\n",
    "#     reference = [ex['cand']]\n",
    "#     candidate = ex['backTrans'].split() \n",
    "#     print(reference)\n",
    "#     print(candidate)\n",
    "#     ex['backTransGram1'] = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
    "#     ex['backTransGram2'] = sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0))\n",
    "#     ex['backTransGram3'] = sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0))\n",
    "#     ex['backTransGram4'] = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "\n",
    "print(test[173])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainPD = pd.DataFrame.from_records(train)\n",
    "# trainPD.to_csv('trainWithAllBackTrans.csv', index=True)  \n",
    "\n",
    "# testPD = pd.DataFrame.from_records(test)\n",
    "testPD.to_csv('testWithAllBackTrans.csv', index=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'according', 'aceh', 'after', 'agency', 'aids', 'all', 'also', 'an', 'and', 'arafat', 'are', 'areas', 'as', 'at', 'be', 'been', 'before', 'beijing', 'between', 'billion', 'bush', 'but', 'by', 'can', 'capital', 'center', 'children', 'china', 'chinese', 'continue', 'control', 'countries', 'country', 'death', 'december', 'disaster', 'during', 'east', 'economic', 'election', 'elections', 'end', 'eu', 'european', 'expected', 'first', 'for', 'forces', 'foreign', 'from', 'global', 'government', 'group', 'had', 'has', 'have', 'he', 'health', 'held', 'his', 'however', 'in', 'including', 'information', 'international', 'iraq', 'iraqi', 'is', 'it', 'its', 'january', 'japan', 'japanese', 'last', 'leader', 'least', 'may', 'medical', 'meeting', 'middle', 'military', 'million', 'minister', 'more', 'most', 'nations', 'new', 'news', 'next', 'no', 'north', 'not', 'november', 'of', 'office', 'officials', 'on', 'one', 'only', 'opposition', 'other', 'out', 'over', 'palestinian', 'party', 'peace', 'people', 'percent', 'political', 'possible', 'powell', 'power', 'president', 'presidential', 'prime', 'provide', 'relations', 'relief', 'russian', 'said', 'scheduled', 'secretary', 'security', 'several', 'should', 'since', 'some', 'south', 'sri', 'state', 'stated', 'states', 'still', 'take', 'that', 'the', 'their', 'there', 'these', 'they', 'this', 'thousands', 'three', 'time', 'to', 'today', 'told', 'tsunami', 'two', 'ukraine', 'un', 'united', 'up', 'victims', 'visit', 'war', 'was', 'we', 'week', 'were', 'when', 'which', 'while', 'who', 'will', 'with', 'world', 'would', 'year', 'years', 'zhao', 'ziyang', '一个', '不过', '世界', '中东', '中国', '中心', '主席', '举行', '乌克兰', '亚洲', '人员', '人民', '今天', '今年', '他们', '以及', '以色列', '伊拉克', '会议', '但是', '俄罗斯', '信息', '儿童', '全球', '公司', '关系', '包括', '十二月', '印尼', '发生', '可以', '可能', '各国', '合作', '告诉', '和平', '因为', '国会', '国家', '国际', '地区', '大选', '安全', '官员', '工作', '已经', '巴勒斯坦', '希望', '总理', '总统', '我们', '所有', '承诺', '指出', '控制', '提供', '援助', '支持', '改革', '攻击', '政府', '政治', '救援', '新闻', '日本', '明年', '显示', '根据', '欧洲', '欧盟', '泰国', '海啸', '灾难', '由于', '病毒', '百分之', '目前', '研究', '系统', '组织', '经济', '继续', '美元', '美国', '联合国', '至少', '行动', '表示', '警告', '认为', '记者', '访问', '这个', '这些', '进行', '选举', '造成', '部队', '重新', '问题', '阿拉法特', '领袖', '鲍尔']\n"
     ]
    }
   ],
   "source": [
    "# 1D - CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy\n",
    "M = 12\n",
    "# train = pandas.DataFrame.from_records(train)\n",
    "trainCorpus = train['data'].values.astype('U')\n",
    "vectorizer = CountVectorizer(min_df=M, binary=True)\n",
    "X = vectorizer.fit_transform(trainCorpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "testCorpus = test['data'].values.astype('U')\n",
    "testVectorizer = CountVectorizer(vocabulary = vectorizer.get_feature_names(),min_df=M, binary=True)\n",
    "testVector = testVectorizer.fit_transform(testCorpus)\n",
    "\n",
    "\n",
    "# print(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testVector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-594bf8c4db0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mtoPredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestVector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnaiveBayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoPredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'qual'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'testVector' is not defined"
     ]
    }
   ],
   "source": [
    "# 1e - Naive Bayes\n",
    "import math\n",
    "def naiveBayes(bagOfWords, targets, toPredict, qual):\n",
    "#   phis\n",
    "#   phi disaster equals disasterRate\n",
    "#   phi not disaster equals 1 - disasterRate\n",
    "    humanRate = np.sum(targets) / len(targets)\n",
    "    \n",
    "    humanIndexes = np.argwhere(targets==1).ravel()\n",
    "    machineIndexes = np.argwhere(targets==0).ravel()\n",
    "    \n",
    "    humanTexts = bagOfWords[humanIndexes]\n",
    "    machineTexts = bagOfWords[machineIndexes]\n",
    "    \n",
    "    humanProbs = np.mean(humanTexts, axis =0) \n",
    "    machineProbs = np.mean(machineTexts, axis =0) \n",
    "    \n",
    "    humanProbs = humanProbs.clip(1e-14, 1-1e-14)\n",
    "    machineProbs = machineProbs.clip(1e-14, 1-1e-14)\n",
    "    \n",
    "#     Prediction\n",
    "    logpyHuman = math.log(humanRate)\n",
    "    logpyMachine= math.log(1 - humanRate)\n",
    "    \n",
    "    logpxyHuman = toPredict * np.log(humanProbs) + (1-toPredict) * np.log(1-humanProbs)\n",
    "    logpxyMachine = toPredict * np.log(machineProbs) + (1-machineProbs) * np.log(1-machineProbs)\n",
    "\n",
    "    logpyxHuman= logpxyHuman.sum(axis=1) + logpyHuman\n",
    "    logpyxMachine = logpxyMachine.sum(axis=1) + logpyMachine\n",
    "    \n",
    "    logpyxHuman = logpyxHuman+qual\n",
    "    logpyxMachine  = logpyxMachine+(1- qual)\n",
    "    \n",
    "    \n",
    "    preds = logpyxHuman > logpyxMachine\n",
    "    return preds\n",
    "\n",
    "def calculateF1(preds, trues, flip = False):\n",
    "    tps = 0 \n",
    "    fps = 0\n",
    "    fns = 0\n",
    "    if flip:\n",
    "        for index, i in enumerate(preds):\n",
    "            if not i and 0 == trues[index]:\n",
    "                tps += 1\n",
    "            if not i and 0 != trues[index]:    \n",
    "                fps += 1\n",
    "            if i and 0 == trues[index]:\n",
    "                fns += 1\n",
    "    else:\n",
    "        for index, i in enumerate(preds):\n",
    "            if i and 1 == trues[index]:\n",
    "                tps += 1\n",
    "            if i and 1 != trues[index]:    \n",
    "                fps += 1\n",
    "            if not i and 1 == trues[index]:\n",
    "                fns += 1\n",
    "    f1 = tps/(tps + 0.5*(fps + fns))\n",
    "    return f1\n",
    "    \n",
    "            \n",
    "    \n",
    "toPredict = testVector.toarray()\n",
    "targets = train['target'].to_numpy()\n",
    "preds = naiveBayes(X.toarray(), targets, toPredict, test['qual'].to_numpy().astype(np.float) )\n",
    "\n",
    "\n",
    "humanF1 = calculateF1(preds, test['target'].to_numpy())\n",
    "machineF1 = calculateF1(preds, test['target'].to_numpy(), True)\n",
    "\n",
    "# Human Score F1\n",
    "\n",
    "avgF1 = (humanF1 + machineF1) / 2\n",
    "print(avgF1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0 1 1 1 0 1 0 0 1 1 1 1 1\n",
      " 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1\n",
      " 0 0 0 0 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1\n",
      " 1 1 0 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 1\n",
      " 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 0]\n",
      "0.766531713900135\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "for i in test['qual'].tolist():\n",
    "#     print(i)\n",
    "    if float(i) >0.52:\n",
    "        preds.append(1)\n",
    "    else:\n",
    "        preds.append(0)\n",
    "print(np.array(preds))\n",
    "\n",
    "humanF1 = calculateF1(preds, test['target'].to_numpy())\n",
    "machineF1 = calculateF1(preds, test['target'].to_numpy(), True)\n",
    "\n",
    "# Human Score F1\n",
    "\n",
    "avgF1 = (humanF1 + machineF1) / 2\n",
    "print(avgF1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1946b65d9688>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# bagOfWords = X.toarray()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# targets = train['target'].to_numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mlogisticPreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestVector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# bagOfWords = X.toarray()\n",
    "# targets = train['target'].to_numpy()\n",
    "clf = LogisticRegression(C=0.1).fit(X.toarray(), targets)\n",
    "logisticPreds = clf.predict(testVector.toarray())\n",
    "\n",
    "temp = np.argpartition(clf.coef_, 10)\n",
    "result_args = temp[:10]\n",
    "\n",
    "topwords = np.argsort(-clf.coef_[0])[:10]\n",
    "for i in topwords:\n",
    "    print(vectorizer.get_feature_names()[i])\n",
    "\n",
    "f1 = calculateF1(logisticPreds, test['target'].to_numpy())\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "巴林公主与美国结婚士兵;令人震惊的婚姻在5年后解散\n",
      "Princess of Bahrain married American soldier; shocking marriage disbanded after 5 years\n",
      "[['Princess', 'of', 'Bahrain', 'married', 'American', 'soldier;', 'shocking', 'marriage', 'disbanded', 'after', '5', 'years']]\n",
      "['bahraini', 'princess', 'marries', 'a', 'u.s.', 'soldier', ';', 'astounding', 'marriage', 'dissolves', 'in', '5', 'years']\n",
      "5.554837769749797e-155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noah/anaconda3/lib/python3.8/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/noah/anaconda3/lib/python3.8/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "巴林的王室成员，在本段中承担军事生涯的风险，因为他们的婚姻就像童话一样，已经成为录像电影的主题。\n",
      "The royal family members of Bahrain take the risk of their military career in this paragraph, because their marriage is like a fairy tale and has become the subject of video movies.\n",
      "[['meri', 'gulf', 'state', 'of', 'bahrain', ',', 'the', 'royal', 'family', 'members', ',', 'who', 'bear', 'the', 'risk', 'of', 'military', 'career', 'in', 'this', 'paragraph', ',', 'as', 'their', 'marriage', 'like', 'a', 'fairy', 'tale', ',', 'has', 'become', 'a', 'video', 'film', 'theme', '.', '']]\n",
      "['The', 'royal', 'family', 'members', 'of', 'Bahrain', 'take', 'the', 'risk', 'of', 'their', 'military', 'career', 'in', 'this', 'paragraph,', 'because', 'their', 'marriage', 'is', 'like', 'a', 'fairy', 'tale', 'and', 'has', 'become', 'the', 'subject', 'of', 'video', 'movies.']\n",
      "0.20170074399470528\n"
     ]
    }
   ],
   "source": [
    "import translators as ts\n",
    "\n",
    "wyw_text = '季姬寂，集鸡，鸡即棘鸡。棘鸡饥叽，季姬及箕稷济鸡。'\n",
    "chs_text = '季姬感到寂寞，罗集了一些鸡来养，鸡是那种出自荆棘丛中的野鸡。野鸡饿了唧唧叫，季姬就拿竹箕中的谷物喂鸡。'\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "\n",
    "# print(ts.alibaba(wyw_text, professional_field='general')) # (\"general\",\"message\",\"offer\")\n",
    "# # property\n",
    "# rs = [ts.tencent(x) for x in [wyw_text, chs_text]]\n",
    "# print(ts._tencent.query_count)\n",
    "# print(dir(ts._tencent))\n",
    "# # requests\n",
    "# print(ts.youdao(wyw_text, sleep_seconds=5, proxies={}, use_cache=True))\n",
    "# # host servic\n",
    "htTest = \"bahraini princess marries a u.s. soldier ; astounding marriage dissolves in 5 years\"\n",
    "chineseTrans = ts.google(htTest, if_use_cn_host=True, to_language ='zh')\n",
    "eng2Trans = ts.google(chineseTrans, if_use_cn_host=True, to_language ='en')\n",
    "print(chineseTrans)\n",
    "print(eng2Trans)\n",
    "\n",
    "reference = [eng2Trans.split(\" \")]\n",
    "candidate = htTest.split(\" \")\n",
    "print(reference)\n",
    "print(candidate)\n",
    "score = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "print(score)\n",
    "\n",
    "\n",
    "\n",
    "mtTest =\"meri gulf state of bahrain , the royal family members , who bear the risk of military career in this paragraph , as their marriage like a fairy tale , has become a video film theme . \"\n",
    "\n",
    "chineseTrans = ts.google(mtTest, if_use_cn_host=True, to_language ='zh')\n",
    "eng2Trans = ts.google(chineseTrans, if_use_cn_host=True, to_language ='en')\n",
    "print(chineseTrans)\n",
    "print(eng2Trans)\n",
    "\n",
    "reference = [mtTest.split(\" \")]\n",
    "candidate = eng2Trans.split(\" \")\n",
    "print(reference)\n",
    "print(candidate)\n",
    "score = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "print(score)\n",
    "\n",
    "\n",
    "\n",
    "# print(ts.bing(wyw_text, if_use_cn_host=False))\n",
    "# # detail result\n",
    "# print(ts.sogou(wyw_text, is_detail_result=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'min_max_scaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-7748fb7ddc20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtestPD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"qual\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_max_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestPD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"qual\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'min_max_scaler' is not defined"
     ]
    }
   ],
   "source": [
    "testPD[[\"qual\"]] = min_max_scaler.fit_transform(testPD[[\"qual\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
